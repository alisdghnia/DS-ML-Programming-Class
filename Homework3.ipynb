{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What is ImageDataGenerator and what does it do? What are the “types” of train_datagen and valid_datagen and how are they used in the code?\n",
    "\n",
    "(b) Explain the architecture of the CNN model used in the code, run it and report the results.\n",
    "\n",
    "(c) Modify the architecture to improve the default model that is in the code. Explain what steps you took to try to improve the accuracy of the default model. (If none of the steps improved the model present the results of the “best” attempt to improve the model, and explain the changes that you tried)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is ImageDataGenerator and what does it do? What are the “types” of train_datagen and valid_datagen and how are they used in the code?\n",
    "\n",
    "ImageDataGenerator is a preprocessing tool to augment the image data. Augmenting data is to create/generate data similar to what is already in our dataset with some differences that can be determined by us. This helps to increase the number of training data that we have to train a more accurate model.\n",
    "\n",
    "What is happening with the train_datagen and valid_datagen is that we are rescaling the image pixels [0,255] to [0,1]. This helps the process of training and predicting faster especially with more data, and additionally and probably most importantly, it will prevent the models and the weights of the cnn model to be skewed towards higher pixels. Normalizes the data in a way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/alisdghnia/Desktop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12642 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=\"FRUITS/Training32\",\n",
    "    target_size=(32, 32),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=5,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4232 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=\"FRUITS/Test32\",\n",
    "    target_size=(32, 32),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=5,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the architecture of the CNN model used in the code, run it and report the results.\n",
    "\n",
    "Mainly with the Conv2D layers, we assign the number of filters, where in each epoch they are trained to detect specific features in the images. The training of the filters is done through minimizing the loss. Then there is the Kernel Size which is (3,3) and is the size of the convolution window. The kernel size is what the filter sizes are and what the images are 'processed' through for the output layer to come out. if there is padding of 'same' or 1 then the output layer will be the same size as input layer which in this case it is (32,32) otherwise if the padding of 1 is not present then the output is going to lose one line from all 4 sides in this case (different filters and kernel sizes will change the output shape).\n",
    "Each time the activation function ReLU is called that turns negative values received in the layer after each convolution to 0 and keeps the positive values. It is a faster process than most other activation functions. Only the positive values staying will emphesize the nodes that result into positive values therefore making the process faster; however, this could lead to nodes losing functionality as the model trains epoch after epoch.\n",
    "Therefore, the use of dropout will help to prevent the nodes from becoming redondent in their work and make sure all nodes matter in the training and testing.\n",
    "Maxpooling then is added to the sequence to downsample the input by grabbing the maximum value from the (2,2) window of the pool size on the layer.\n",
    "\n",
    "Repeated X2\n",
    "\n",
    "Then the values are flattened to convert the data to a 1-d array in order to create a feature vector and basically use the feature vector to then dense it into the number of classification outputs we have at the end. (while also adding some activation and drop out in the middle).\n",
    "And softmax activation is for the outputs of more than 2 categories.\n",
    "\n",
    "Lastly, Adam optimizer with a learning rate of 0.0001 is used as the optimizer for the learning of the model and uses the adam algorithm to calculate the weights through backpropagation.\n",
    "\n",
    "Loss function is categorical crossentropy which is a common loss function used for multi-class classification problems.\n",
    "\n",
    "\n",
    "Output will be 25 indicating which of the 25 keys in the dictionary (which is all the categories of the items in the images) the image is showing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                12825     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 25)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,258,553\n",
      "Trainable params: 1,258,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 23:59:47.066031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/alisdghnia/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# initiate adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.modules['Image'] = Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 12/100 [==>...........................] - ETA: 0s - loss: 0.2096 - accuracy: 0.9500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/qq7pw_fx125c812gfv1wl_180000gn/T/ipykernel_23926/2210581391.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 11ms/step - loss: 0.2807 - accuracy: 0.9020 - val_loss: 0.3355 - val_accuracy: 0.8800\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.3685 - accuracy: 0.8780 - val_loss: 0.3527 - val_accuracy: 0.8660\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.3739 - accuracy: 0.8760 - val_loss: 0.3927 - val_accuracy: 0.8520\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2371 - accuracy: 0.9280 - val_loss: 0.2971 - val_accuracy: 0.8780\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2628 - accuracy: 0.9080 - val_loss: 0.2219 - val_accuracy: 0.9160\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2159 - accuracy: 0.9320 - val_loss: 0.3222 - val_accuracy: 0.9020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.2270 - accuracy: 0.9080 - val_loss: 0.3316 - val_accuracy: 0.8680\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.2304 - accuracy: 0.9060 - val_loss: 0.3011 - val_accuracy: 0.9000\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2641 - accuracy: 0.9120 - val_loss: 0.3965 - val_accuracy: 0.8580\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1945 - accuracy: 0.9215 - val_loss: 0.3012 - val_accuracy: 0.8960\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2331 - accuracy: 0.9240 - val_loss: 0.2944 - val_accuracy: 0.9060\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1701 - accuracy: 0.9340 - val_loss: 0.3554 - val_accuracy: 0.8460\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1808 - accuracy: 0.9440 - val_loss: 0.3023 - val_accuracy: 0.8840\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2741 - accuracy: 0.9120 - val_loss: 0.2968 - val_accuracy: 0.8780\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1973 - accuracy: 0.9240 - val_loss: 0.2185 - val_accuracy: 0.9300\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1490 - accuracy: 0.9540 - val_loss: 0.2382 - val_accuracy: 0.9300\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1931 - accuracy: 0.9320 - val_loss: 0.2283 - val_accuracy: 0.9160\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1543 - accuracy: 0.9420 - val_loss: 0.3021 - val_accuracy: 0.9020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1906 - accuracy: 0.9280 - val_loss: 0.2603 - val_accuracy: 0.9180\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1377 - accuracy: 0.9600 - val_loss: 0.2421 - val_accuracy: 0.9020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1492 - accuracy: 0.9480 - val_loss: 0.1881 - val_accuracy: 0.9280\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1357 - accuracy: 0.9460 - val_loss: 0.1678 - val_accuracy: 0.9260\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1466 - accuracy: 0.9400 - val_loss: 0.1960 - val_accuracy: 0.9340\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1874 - accuracy: 0.9320 - val_loss: 0.2022 - val_accuracy: 0.9300\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1498 - accuracy: 0.9460 - val_loss: 0.2098 - val_accuracy: 0.9280\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1788 - accuracy: 0.9400 - val_loss: 0.2973 - val_accuracy: 0.8860\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1726 - accuracy: 0.9320 - val_loss: 0.1670 - val_accuracy: 0.9400\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1061 - accuracy: 0.9700 - val_loss: 0.2689 - val_accuracy: 0.9060\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1166 - accuracy: 0.9620 - val_loss: 0.1225 - val_accuracy: 0.9380\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0920 - accuracy: 0.9680 - val_loss: 0.1891 - val_accuracy: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc86165c6d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=30,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict a single image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"FRUITS/Test32/Apple Braeburn/3_100.jpg\")\n",
    "\n",
    "\n",
    "#CV2 reads an image in BGR format. We need to convert it to RGB\n",
    "b,g,r = cv2.split(img)       # get b,g,r\n",
    "rgb_img = cv2.merge([r,g,b])     # switch it to rgb\n",
    "\n",
    "\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to have the image as a tensor of rank 4\n",
    "#Because we trained the model as (batch_size, height, width, channel)\n",
    "#Here, batch_size will be one\n",
    "\n",
    "#Also divide the image values by 255 to normalize\n",
    "\n",
    "img_rank4 = np.expand_dims(rgb_img/255, axis=0)\n",
    "\n",
    "model.predict(img_rank4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(model.predict(img_rank4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT SHOWN IN THE TUTORIAL VIDEO:\n",
    "# We can predict the class directly using the following function:\n",
    "\n",
    "# model.predict_classes(img_rank4)\n",
    "predict_x=model.predict(img_rank4) \n",
    "classes_x=np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple Braeburn': 0,\n",
       " 'Apple Golden 1': 1,\n",
       " 'Apple Golden 2': 2,\n",
       " 'Apple Golden 3': 3,\n",
       " 'Apple Granny Smith': 4,\n",
       " 'Apple Red 1': 5,\n",
       " 'Apple Red 2': 6,\n",
       " 'Apple Red 3': 7,\n",
       " 'Apple Red Delicious': 8,\n",
       " 'Apple Red Yellow': 9,\n",
       " 'Apricot': 10,\n",
       " 'Avocado': 11,\n",
       " 'Avocado ripe': 12,\n",
       " 'Banana': 13,\n",
       " 'Banana Red': 14,\n",
       " 'Cactus fruit': 15,\n",
       " 'Cantaloupe 1': 16,\n",
       " 'Cantaloupe 2': 17,\n",
       " 'Carambula': 18,\n",
       " 'Cherry 1': 19,\n",
       " 'Cherry 2': 20,\n",
       " 'Cherry Rainier': 21,\n",
       " 'Cherry Wax Black': 22,\n",
       " 'Cherry Wax Red': 23,\n",
       " 'Cherry Wax Yellow': 24}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT SHOWN IN THE TUTORIAL VIDEO:\n",
    "# We can retrieve the class labels from the train_generator:\n",
    "\n",
    "label_map = (train_generator.class_indices)\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple Braeburn'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT SHOWN IN THE TUTORIAL VIDEO:\n",
    "# We can retrieve the class label of the prediction:\n",
    "\n",
    "# list(label_map.keys())[model.predict_classes(img_rank4)[0]]\n",
    "\n",
    "list(label_map.keys())[classes_x[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the architecture to improve the default model that is in the code. Explain what steps you took to try to improve the accuracy of the default model. (If none of the steps improved the model present the results of the “best” attempt to improve the model, and explain the changes that you tried)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 30, 30, 64)        36928     \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 30, 30, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 15, 15, 128)       73856     \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 15, 15, 128)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 13, 13, 128)       147584    \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 13, 13, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               1179904   \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 25)                6425      \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 25)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,446,489\n",
      "Trainable params: 1,446,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.05))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.15))\n",
    "\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# initiate adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(lr=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/qq7pw_fx125c812gfv1wl_180000gn/T/ipykernel_23926/2210581391.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 25ms/step - loss: 2.7122 - accuracy: 0.1820 - val_loss: 1.8112 - val_accuracy: 0.3840\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 1.5373 - accuracy: 0.4820 - val_loss: 1.3191 - val_accuracy: 0.6280\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 2s 25ms/step - loss: 0.9226 - accuracy: 0.6860 - val_loss: 1.1677 - val_accuracy: 0.5780\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.6680 - accuracy: 0.7860 - val_loss: 0.7538 - val_accuracy: 0.7580\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.5169 - accuracy: 0.8260 - val_loss: 0.8999 - val_accuracy: 0.6660\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.4632 - accuracy: 0.8540 - val_loss: 0.8746 - val_accuracy: 0.7180\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.4338 - accuracy: 0.8480 - val_loss: 0.5246 - val_accuracy: 0.8320\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.2483 - accuracy: 0.9200 - val_loss: 0.4377 - val_accuracy: 0.8520\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.2254 - accuracy: 0.9180 - val_loss: 0.6677 - val_accuracy: 0.7700\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.2940 - accuracy: 0.8860 - val_loss: 0.7435 - val_accuracy: 0.7680\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.3105 - accuracy: 0.8920 - val_loss: 0.4486 - val_accuracy: 0.8240\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.1957 - accuracy: 0.9320 - val_loss: 0.5658 - val_accuracy: 0.8540\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.1377 - accuracy: 0.9440 - val_loss: 0.4076 - val_accuracy: 0.8540\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 2s 25ms/step - loss: 0.1656 - accuracy: 0.9360 - val_loss: 0.3428 - val_accuracy: 0.8800\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.1187 - accuracy: 0.9620 - val_loss: 0.6454 - val_accuracy: 0.8460\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.2099 - accuracy: 0.9260 - val_loss: 0.5675 - val_accuracy: 0.8080\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.1147 - accuracy: 0.9620 - val_loss: 0.5744 - val_accuracy: 0.8240\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.2081 - accuracy: 0.9360 - val_loss: 0.6944 - val_accuracy: 0.7900\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 2s 25ms/step - loss: 0.1865 - accuracy: 0.9360 - val_loss: 0.5195 - val_accuracy: 0.8140\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 2s 25ms/step - loss: 0.0702 - accuracy: 0.9700 - val_loss: 0.2578 - val_accuracy: 0.9240\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.1976 - accuracy: 0.9360 - val_loss: 0.4851 - val_accuracy: 0.8360\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0936 - accuracy: 0.9680 - val_loss: 0.2705 - val_accuracy: 0.9140\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.1744 - accuracy: 0.9460 - val_loss: 0.3491 - val_accuracy: 0.8760\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0864 - accuracy: 0.9680 - val_loss: 0.3131 - val_accuracy: 0.9200\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0952 - accuracy: 0.9660 - val_loss: 0.2980 - val_accuracy: 0.8820\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0703 - accuracy: 0.9700 - val_loss: 0.5042 - val_accuracy: 0.8720\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0698 - accuracy: 0.9780 - val_loss: 0.3127 - val_accuracy: 0.9100\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.2240 - accuracy: 0.9280 - val_loss: 0.6571 - val_accuracy: 0.8160\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.3012 - accuracy: 0.9060 - val_loss: 0.2974 - val_accuracy: 0.9100\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0942 - accuracy: 0.9620 - val_loss: 0.1847 - val_accuracy: 0.9160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc8857582e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=30,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Originally in this notebook the first model was only trained with the epoch set at 5 which did not create a model that could predict very accurately. I changed the batch size in the beginning and increased the number of epoch to 30. Lowered the dropout and increased the number of filters. Basically I overfit the heck out of the model just to find a really high accuracy which ended with 96% accuracy! However, the Validation Loss and Accuracy could even be worked on more if we augment the training data more and also maybe make the architecture of the model more complex. But that would be pointless as this is already a great model to predict the fruits in the images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7005d3446f2fd2660246ee79c29ee8adee1370e0edb9fb24f32131585339a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
